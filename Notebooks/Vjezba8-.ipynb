{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9652c95a-ad56-4597-99bd-44619083306f",
   "metadata": {},
   "source": [
    "# Inicijalizacija parametara modela \n",
    "\n",
    "\n",
    "<center><img src=\"Images/V8_banner.png\" width=\"700\" height=\"700\"/></center>\n",
    "\n",
    "\n",
    "Tijekom optimizacije pokazano je da optimizator ovisi uvelike o broju epoha, stopi učenja $\\alpha$, momentumu, ali i o početnim vrijednostima parametara modela $\\Theta$. U ovoj vježbi istražiti će se nestajući gradijenti, eksplozija gradijenata, saturirani neuroni te utjecaj prevelike pristranosti i varijance.\n",
    "\n",
    "---\n",
    "\n",
    "### Važnost inicijalizacije težina\n",
    "\n",
    "Prije nego što prijeđemo na Xavierov postupak inicijalizacije, važno je razumjeti zašto je inicijalizacija težina ključna u dubokom učenju.\n",
    "\n",
    "#### 1. Nestajanje i eksplodiranje gradijenata\n",
    "Problem **nestajanja gradijenata** pojavljuje se kada gradijenti tijekom treniranja postanu iznimno mali. Zbog toga mreža uči vrlo sporo ili uopće ne uči, što je osobito izraženo kod dubokih mreža.\n",
    "\n",
    "Suprotno tome, kod **eksplodiranja gradijenata** vrijednosti gradijenata postaju vrlo velike, što dovodi do nestabilnog i neučinkovitog treniranja te može uzrokovati divergenciju modela.\n",
    "\n",
    "Oba problema otežavaju ili potpuno onemogućuju uspješno treniranje dubokih neuronskih mreža.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Problem prenaučenosti (overfitting)\n",
    "Neuronske mreže, posebno duboke, imaju velik kapacitet za učenje vrlo složenih obrazaca iz podataka. Taj kapacitet, međutim, čini ih sklonima **prenaučenosti**.\n",
    "\n",
    "Inicijalizacija težina ne sprječava izravno overfitting, ali **neizravno doprinosi** boljoj generalizaciji tako što modelu omogućuje da započne treniranje sa stabilno skaliranim težinama. Na taj se način smanjuju problemi poput nestajanja gradijenata i saturacije neurona.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Saturacija aktivacijskih funkcija\n",
    "**Saturacija** se događa kada izlaz aktivacijske funkcije postane vrlo blizak svom minimalnom ili maksimalnom mogućem iznosu za širok raspon ulaznih vrijednosti.\n",
    "\n",
    "U tom stanju:\n",
    "- aktivacijska funkcija postaje neosjetljiva na promjene ulaza  \n",
    "- gradijent joj se približava nuli  \n",
    "- učenje postaje sporo ili se potpuno zaustavlja  \n",
    "\n",
    "Pravilnom inicijalizacijom težina osigurava se da početne aktivacije budu u uravnoteženom rasponu, što pomaže u izbjegavanju saturacije i problema s gradijentima.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823df83b-9429-4fe8-a236-a142823011c1",
   "metadata": {},
   "source": [
    "# Inicijalizacija parametara\n",
    "\n",
    "<font color='red'>\n",
    "\n",
    "\n",
    "## Zadatak\n",
    "\n",
    "<left><img src=\"Images/Zadatak.png\" width=\"70\" height=\"70\"/></left>\n",
    "</font>\n",
    "\n",
    "Parametre/težine na početku neuornske mreže je potrebno inicijalizirati. Navedite razloge koji su potenicjalni problemi pogrešne inicijalizacije parametara te kako se isti reflektiraju na broj epoha, stopu učenja optimizatora i konvergenciju/divergenciju neuronske mreže.\n",
    "\n",
    "---\n",
    "\n",
    "Inicjalizacija parametara neuronskih mreža tipično se vrši pomoću dviju metoda:\n",
    "\n",
    "* **Xavier** (Golort) nazvana po autoru Xavieru Glorotu koji ju objavio u [znansvenom radu](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf).\n",
    "* **He** (Kaming) nazvana po autoru He Kammingu koji je je objavio u [znanstvenom radu](https://arxiv.org/abs/1502.01852)\n",
    "\n",
    "---\n",
    "\n",
    "## Xavier inicijalizacija\n",
    "\n",
    "Xavier u svom radu istraživao je do tada široko prihvaćeno pravilo za inicijalizaciju težina neruonskih mreža odabirom slučajnih vrijednosti iz uniformne distribucije koja se kreće između -1 i 1. Nakon ove slučajne inicijalizacije, težine se zatim smanjuju za faktor 1 podijeljen s kvadratnim korijenom broja ulaznih jedinica (označenog kao 'n').\n",
    "\n",
    "Cilj je riješiti problem održavanja varijance u prolasku neuronske mreže naprijed i natrag, posebno pri korištenju određenih aktivacijskih funkcija poput **hiperboličkog tangenta (tanh)** i **logističkog sigmoida**. Bez obzira na to koliko ulaznih veza neuron u sloju ima, varijanca njegovog izlaza trebala bi biti približno ista. Ovo svojstvo pomaže u sprječavanju problema nestajanja ili eksplozije gradijenta koji se može pojaviti ako se varijance drastično promijene između slojeva. Slično tome, varijanca gradijenta tijekom propagacije unatrage također bi trebala biti otprilike konstantna bez obzira na broj neurona u sljedećem sloju. To pomaže u održavanju stabilne dinamike učenja.\n",
    "\n",
    "U Xavierovoj inicijalizaciji, ključni faktor je **broj ulaza i izlaza u slojevima**, a ne toliko metoda randomizacije. Cilj je održati varijancu u granicama koje omogućuju učinkovito učenje s različitim aktivacijskim funkcijama. Postoje dvije inačice Xavier inicijalizacije:\n",
    "\n",
    "---\n",
    "\n",
    "#### Uniformna Xavier inicijalizacija\n",
    "Težine možemo inicijalizirati tako da ih generiramo iz uniformne distribucije unutar specifičnog raspona koji se određuje formulom:\n",
    "\n",
    "\\begin{align}\n",
    "x = \\sqrt{\\frac{6}{n_{\\text{inputs}} + n_{\\text{outputs}}}}\n",
    "\\end{align}\n",
    "\n",
    "U kojoj se $x$ računa pomoću naveden formule, $n_inputs$ je broj neurona u ulaznom sloju, dok je $n_outputs$ broj neurona u izlaznom sloju. Za svaku težinu $w$ u sloju uzrokuje se slučajna vrijednost iz unifromne distribucije $w \\in [-x, x]$. Ovim ograničavanjem težina kontrolira se varijanca početnih težina.\n",
    "\n",
    "---\n",
    "\n",
    "#### Normalna Xavier inicijalzacija\n",
    "Ova inicijalizacija postavlja početne težine uzorkovanjem vrijednosti iz normalne distribucije sa srednjom vrijednošću $\\mu = 0$ i standardnom devijacijom određenom formulom:\n",
    "\n",
    "\\begin{align}\n",
    "\\sigma = \\sqrt{\\frac{2}{n_{\\text{inputs}} + n_{\\text{outputs}}}}\n",
    "\\end{align}\n",
    "\n",
    "gdje se $\\sigma$ računa pomoću navedene formule, $n_inputs$ je broj neurona u ulaznom sloju, dok je $n_outputs$ broj neurona u izlaznom sloju. Drugim riječima $w = \\mathcal{N}(\\mu, \\sigma)$. Odabir između normalne i unifomne Xavier inicijalzacije ovisi o arhitekturi neuronske mreže i korištenim aktiviacijskim funkcijama. \n",
    "\n",
    "---\n",
    "\n",
    "#### Kada koristiti koju?\n",
    "\n",
    "Načelno, i nema prevelike razlike osim u ReLU i LeakyReLU!\n",
    "\n",
    "| Aktivacijska funkcija | Preporučena inicijalizacija | Objašnjenje |\n",
    "|------------------------|------------------------------|-------------|\n",
    "| **tanh**               | **Xavier Uniform**           | Najstabilnije, manje ekstremne vrijednosti. |\n",
    "| **sigmoid**            | **Xavier Uniform**           | Stabilizira varijancu, izbjegava saturaciju. |\n",
    "| **softmax (ulazi)**    | **Xavier Uniform**           | Dobro radi za klasifikacijske slojeve. |\n",
    "| **Linear (bez aktivacije)** | Xavier Uniform ili Xavier Normal | Obje opcije su dobre, Uniform nešto stabilniji. |\n",
    "| **tanh / sigmoid (specifični modeli npr. VAE decoder, Difuzija)** | **Xavier Normal** | Gaussian distribucija poželjna zbog modeliranja šuma. |\n",
    "| **ReLU**               | ❌ Ne Xavier → ✔️ **Kaiming/He** | Xavier daje premalu varijancu za ReLU. |\n",
    "| **LeakyReLU / ELU**    | ❌ Ne Xavier → ✔️ **Kaiming/He** | Isto kao i ReLU – potrebna veća varijanca. |\n",
    "\n",
    "\n",
    "## Kaming He inicijalizacija\n",
    "\n",
    "Kaiming inicijalizacija, poznata i kao *Kaiming He inicijalizacija* ili *He normal inicijalizacija*, tehnika je inicijalizacije težina u umjetnim neuronskim mrežama. Ovu su metodu predstavili Kaiming He, Xiangyu Zhang, Shaoqing Ren i Jian Sun u radu: **\"Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification\"**\n",
    "\n",
    "Glavna motivacija Kaiming inicijalizacije jest rješavanje problema **nestajanja** ili **eksplodiranja gradijenata** koji se mogu pojaviti tijekom treniranja dubokih neuronskih mreža. U dubokim mrežama, posebno onima koje koriste **ReLU** aktivacijsku funkciju, tradicionalne metode inicijalizacije (npr. nasumična normalna ili Xavier inicijalizacija) mogu uzrokovati da gradijenti nestanu ili eksplodiraju tijekom propagacije unatrag.\n",
    "\n",
    "Kaiming inicijalizacija definira težine kao slučajne brojeve iz normalne (Gaussove) distribucije:\n",
    "\n",
    "\\begin{align}\n",
    "W \\sim \\mathcal{N}\\left(0, \\frac{2}{n}\\right)\n",
    "\\end{align}\n",
    "\n",
    "gdje je $n$ broj ulaznih neurona u sloj. Faktor 2 u varijanci posebno je prilagođen za ReLU aktivaciju. Za druge aktivacijske funkcije (npr. sigmoid, tanh) prikladnije su druge strategije inicijalizacije.\n",
    "\n",
    "\n",
    "#### Prednosti\n",
    "\n",
    "Kroz detaljitu analizu svojstava ReLU funkcije i izazova koje predstavljaju nestajući i eksplodirajući gradijenti, Kaiming He i njegove kolege osmislili su metodu inicijalizacije koja je postala kamen temeljac za inicijalizaciju težina u dubokim neuronskim mrežama. **Prednosti Kaimingove metode inicijalizacije su:**\n",
    "\n",
    "* **Ublažavanje problema nestajućih i eksplodirajućih gradijenata:**  \n",
    "  Kaiming He inicijalizacija pomaže ublažiti probleme nestajućih i eksplodirajućih gradijenata koji mogu otežati treniranje dubokih mreža. (Xavier isto)\n",
    "\n",
    "* **Očuvanje varijance:**  \n",
    "  Metoda inicijalizacije osmišljena je tako da očuva varijancu težina, osobito tijekom propagacije unaprijed. To pomaže zadržati odgovarajuću skalu aktivacija kroz slojeve, sprječavajući pojavu presitnih ili prevelikih vrijednosti. (Xavier isto)\n",
    "\n",
    "* **Učinkovito treniranje dubljih mreža:**  \n",
    "  Pokazalo se da Kaiming He inicijalizacija doprinosi učinkovitijem treniranju dubljih mreža. Duboke neuronske mreže s mnogo slojeva mogu imati koristi od ove metode inicijalizacije kako bi se osiguralo djelotvorno propagiranje informacija i gradijenata kroz mrežu.\n",
    "\n",
    "* **Prilagođenost ReLU aktivacijskoj funkciji:**  \n",
    "  Kaiming He inicijalizacija prilagođena je ReLU aktivacijskim funkcijama, koje se široko koriste u dubokom učenju. U obzir uzima karakteristike ReLU-a, poput ne-saturacije za pozitivne ulaze, kako bi postavila odgovarajuće početne težine.\n",
    "\n",
    "* **Empirijski uspjeh na raznim zadacima:**  \n",
    "  Empirijske studije dosljedno su pokazale učinkovitost Kaiming He inicijalizacije u raznim zadacima računalnog vida i obrade prirodnog jezika. Postala je jedna od najčešće korištenih strategija inicijalizacije među praktičarima koji rade s dubokim neuronskim mrežama.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448922e2-ea47-48e5-82e3-b3cdff9d0e60",
   "metadata": {},
   "source": [
    "## Nestajući gradijenti\n",
    "\n",
    "**Nestajući gradijent** (*vanishing gradient*) je problem u dubokim neuronskim mrežama koji nastaje kada tijekom **propagacije prema unatrag** gradijenti postanu izrazito mali.\n",
    "\n",
    "Posljedica nestajućeg gradijenta je:\n",
    "\n",
    "* vrlo sporog učenja u ranim slojevima  \n",
    "* gotovo nikakve promjene težina\n",
    "* nemogućnosti mreže da nauči dublje obrasce  \n",
    "\n",
    "Najčešći uzrok su aktivacijske funkcije poput **sigmoida** ili **tanh**, koje “stišću” vrijednosti u uzak raspon. Kada se to ponavlja kroz mnogo slojeva, gradijenti postupno postaju gotovo nula.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Eksplodirajući gradijenti\n",
    "\n",
    "**Eksplodirajući gradijenti** (*exploding gradients*) su problem u dubokim neuronskim mrežama kada tijekom **propagacije prema unatrag** gradijenti postanu **preveliki**.\n",
    "\n",
    "To uzrokuje:\n",
    "\n",
    "* nagle, nestabilne promjene težina  \n",
    "* “skakanje” modela po prostoru rješenja  \n",
    "* nemogućnost konvergencije tijekom treniranja  \n",
    "* ponekad i numeričke greške (npr. `NaN` vrijednosti)\n",
    "\n",
    "Do eksplodirajućih gradijenata najčešće dolazi u vrlo dubokim mrežama ili u rekurentnim neuronskim mrežama (RNN), gdje se gradijenti multipliciraju kroz mnogo koraka, pa mogu narasti do ekstremno velikih vrijednosti.\n",
    "\n",
    "---\n",
    "\n",
    "## Zasićeni neuroni\n",
    "\n",
    "**Zasićeni neuroni** (*saturated neurons*) pojavljuju se kada je izlaz neurona “zapeo” u području gdje se aktivacijska funkcija gotovo ne mijenja — najčešće na svojim **gornjim ili donjim granicama**.\n",
    "\n",
    "To dovodi do:\n",
    "\n",
    "- **gradijenti postaju gotovo nula** → neuron prestaje učiti  \n",
    "- vrlo spore ili nikakve promjene težina  \n",
    "- gubitka sposobnosti mreže da modelira složene obrasce  \n",
    "- usporenog ili blokiranog treniranja u dubokim mrežama\n",
    "\n",
    "Zasićenje se najčešće događa kod aktivacijskih funkcija **sigmoid** i **tanh**, jer imaju “ravne” krajeve.  \n",
    "Kada ulaz padne u ta ravna područja, derivacija je gotovo nula — pa neuron ne šalje korisne gradijente unatrag.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a115e7-c483-42f7-96c3-5700dc667b6f",
   "metadata": {},
   "source": [
    "<font color='green'>\n",
    "\n",
    "\n",
    "## Primjer\n",
    "\n",
    "<left><img src=\"Images/Primjer.png\" width=\"70\" height=\"70\"/></left>\n",
    "\n",
    "</font>\n",
    "\n",
    "Primjeri navedenih problema dati će se na skupu podataka **MNIST** koji je dostupan na idućoj [poveznici](https://www.kaggle.com/datasets/hojjatk/mnist-dataset). Ukratko, za slike znamenki od 0 do 9 potrebno je predvdjeti koja je znamenka nacrtana na slici. \n",
    "\n",
    "Slike su male - dimenzija $28\\times28$ što omogućuje da se slika \"spljošti\" u jedan niz brojeva te se koristi poptpuno povezana neuronska mrežu sa 10 predikcijskih neurona -- po jedan za svaku klasu.\n",
    "\n",
    "Kao optimizator se koristi **Adam**, dok je funkcija cilja **unakrsna entropija**.\n",
    "\n",
    "Idući proramski kod kreira skup podataka, i stvara model prema zadanim parametrima. Proučite programski kod te razmislite zašto smo podijelili skup podataka u podskupove **Train**, **Valid** i **Test**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa02ea7f-ca9e-4adc-b92c-3fc9e81a8d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Učitavanje programskih knjižnica\n",
    "from Skripte.Vjezba8.dataloader import mnist_dataset\n",
    "\n",
    "# Kreiranje skupova podataka\n",
    "_train_dataset = mnist_dataset(subset = \"train\")\n",
    "_valid_dataset = mnist_dataset(subset = \"valid\")\n",
    "_test_dataset = mnist_dataset(subset = \"test\")\n",
    "\n",
    "# Broj elemenata\n",
    "print(f\"Train: {len(_train_dataset)}, Valid: {len(_valid_dataset)}, Test:{len(_test_dataset)}\")\n",
    "\n",
    "_train_dataset.visualise()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30835ec9-4536-4b68-af5b-aae4eda635f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Učitavanje programskih knjižica\n",
    "from Skripte.Vjezba8.models import DeepMLPNet, print_model_summary\n",
    "\n",
    "## Model\n",
    "_model = DeepMLPNet(\n",
    "    input_dim=28*28,\n",
    "    hidden_dim=256,\n",
    "    num_layers=5,\n",
    "    num_classes=10,\n",
    "    init_scale=0.01,        \n",
    "    activation=\"sigmoid\",         \n",
    "    init_type=\"xavier_uniform\",   \n",
    "    init_constant_value=0.05,     \n",
    ")\n",
    "\n",
    "# Ispis modela\n",
    "print_model_summary(_model, input_dim= (32,28*28))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea40d7c-92c6-4616-b4fc-812986d7261e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<font color='red'>\n",
    "\n",
    "\n",
    "## Zadatak\n",
    "\n",
    "<left><img src=\"Images/Zadatak.png\" width=\"70\" height=\"70\"/></left>\n",
    "\n",
    "</font>\n",
    "\n",
    "Sljedeći programski kod omogućuje podešavanje brojnih parametara treninga modela koji se inače zadaju konfiguracijskim datotekama. Parametri koji se mogu mijenjati su redom:\n",
    "\n",
    "- **Hidden dim** – broj neurona u svakom skrivenom sloju.\n",
    "- **# slojeva (num_layers)** – koliko skrivenih slojeva ima mreža.\n",
    "- **# epoha (num_epochs)** – broj prolazaka kroz cijeli trening skup.\n",
    "- **Init scale** – dodatno skaliranje inicijaliziranih težina (npr. 1.0, 0.1, 0.01).\n",
    "- **Aktivacija (activation)** – odabir aktivacijske funkcije: sigmoid, relu, tanh.\n",
    "- **Init tip (init_type)** – tip inicijalizacije težina: xavier_uniform, xavier_normal, kaiming_uniform, zeros, constant.\n",
    "- **Const val (init_constant_value)** – vrijednost težina ako je init_type=\"constant\".\n",
    "- **LR (learning_rate)** – learning rate Adam optimizatora.\n",
    "- **Train BS (train_batch_size)** – veličina batcha za treniranje.\n",
    "- **Valid ratio** – udio validacijskog skupa (npr. 0.1 = 10%).\n",
    "- **Max batch/ep** – maksimalan broj batch-eva po epohi (radi ubrzavanja).\n",
    "\n",
    "Vaš je zadatak odgovoriti na sljedeća pitanja:\n",
    "\n",
    "* Odredite kojem scenariju pripada trening/model sa sljedećem parametrima:\n",
    "    - `hidden_dim = 256`\n",
    "    - `# slojeva = 10`\n",
    "    - `Init scale = 0.01`\n",
    "    - `Aktivacija = \"sigmoid\"`\n",
    "    - `Init tip = \"xavier_uniform\"`\n",
    "    - `Const val = 0.01`\n",
    "    - `# epoha = 8`\n",
    "    - `LR = 1e-3`\n",
    "    - `Train BS = 150`\n",
    "    - `Valid ratio = 0.10`\n",
    "    - `Max batch = 150`\n",
    "\n",
    "* Odredite kojem scenariju pripada trening/model sa sljedećem parametrima:\n",
    "    - `hidden_dim = 256`\n",
    "    - `# slojeva = 15`\n",
    "    - `Init scale = 10.0`         \n",
    "    - `Aktivacija = \"relu\"`\n",
    "    - `Init tip = \"xavier_normal\"`\n",
    "    - `Const val = 0.01`          \n",
    "    - `# epoha = 15`              \n",
    "    - `LR = 1e-2`                 \n",
    "    - `Train BS = 150`            \n",
    "    - `Valid ratio = 0.10`\n",
    "    - `Max batch = 150`\n",
    "\n",
    "* Što će se dogoditi ako su sve vrijednosti inicijalizirane na $0$ ili konstantna vrijednost $0.05$. Ponudite odgovor zašto se dešava navedeni scenarij. Postavke modela su sljedeće:\n",
    "\n",
    "    - `hidden_dim = 256`\n",
    "    - `# slojeva = 8`\n",
    "    - `Init scale = 0.001`         \n",
    "    - `Aktivacija = \"relu\"`\n",
    "    - `Init tip = \"zeros\" ili \"constant`\n",
    "    - `Const val = 0.05`          \n",
    "    - `# epoha = 8`              \n",
    "    - `LR = 1e-3`                 \n",
    "    - `Train BS = 150`            \n",
    "    - `Valid ratio = 0.10`\n",
    "    - `Max batch = 150`\n",
    "\n",
    "* Koja aktivacijska funkcija lakše podlježe nestajućim gradijentima, a koja eksploziji gradijenata i zašto?\n",
    "\n",
    "* Podesite parametre neuronske mreže tako da ostvarite preciznost F1-Score-a preko $90\\%$ koristeći a) Sigmoid, b) learning rate $0.1$, c) samo jedan sloj. Steknite iskustvo podešavanja parametara i odgovorite kako svaki od parametara utječe na trening neuronske mreže."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031b88a3-3c90-4fe3-bb27-6c83ffd836e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Učitavanje programskih knjižica\n",
    "from Skripte.Vjezba8.widget import deep_mlp_training_widget\n",
    "\n",
    "_ui = deep_mlp_training_widget()\n",
    "_ui"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99a8b36-3e0e-4701-8d6d-4fd41b5b93eb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624c610d-eece-4a2e-b286-fd56140f5999",
   "metadata": {},
   "source": [
    "## Pristranost i varijanca\n",
    "\n",
    "U scenariju **visoke pristranosti (high bias)** model ima premali kapacitet i ne može naučiti osnovne obrasce u podacima.  \n",
    "Na grafu se vidi da **funkcije gubitka nad traning i nad validacijskim skupom podataka visoki**, te da se smanjuju sporo i gotovo paralelno.  \n",
    "To znači da model *podjednako loše generalizira i na trening i na validiacijskom skup*, što je tipični znak **underfittinga**.\n",
    "\n",
    "S druge strane, u scenariju **visoke varijance (high variance)** model ima prevelik kapacitet i previše se prilagodi treninging podacima.  \n",
    "Na grafu se vidi **da funkcija gubitka nad trainining skupu podataka opada**, dok se **na validacijskom skupu nakon određenog broja epoha počinje povećavati**. To znači da je model naučio detalje i šum iz skupa za treniranje, ali loše **generalizira** na neviđenim primjerima – tipičan znak **overfittinga**.\n",
    "\n",
    "| Svojstvo                     | Visoka pristranost (High Bias)                         | Visoka varijanca (High Variance)                          |\n",
    "|------------------------------|----------------------------------------------------------|------------------------------------------------------------|\n",
    "| Treniranje (Train loss)      | Visok i sporo opada                                      | Vrlo nizak, često ide gotovo do 0                          |\n",
    "| Validacija (Valid loss)      | Visok, vrlo sličan train lossu                           | Početno se smanjuje, a zatim **raste**                     |\n",
    "| Razlika train–valid          | Mala                                                     | Velika                                                     |\n",
    "| Generalizacija               | Loša (model preslab)                                     | Loša (model prejak)                                       |\n",
    "| Kapacitet modela             | Prenizak (too simple)                                    | Previsok (too complex)                                     |\n",
    "| Tipični uzrok                | Malo slojeva, malo neurona, prekratko treniranje         | Mnogi slojevi, mnogo neurona, predugo treniranje           |\n",
    "| Tipična posljedica           | **Underfitting**                                         | **Overfitting**                                            |\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<font color='red'>\n",
    "\n",
    "\n",
    "## Zadatak\n",
    "\n",
    "<left><img src=\"Images/Zadatak.png\" width=\"70\" height=\"70\"/></left>\n",
    "\n",
    "</font>\n",
    "\n",
    "Za slijdeće dvije neuronske mreže sa proučite koja demonstrira slučaj visoke pristranosti, a koje visoke varijance. Pokušajte postići iste scenarije pomoću drugih parametara, primijerice mijenjajte veličinu skupa podataka.\n",
    "\n",
    "Mreža 1.:\n",
    "- `hidden_dim = 4`\n",
    "- `# slojeva = 2`\n",
    "- `Init scale = 1.0`         \n",
    "- `Aktivacija = \"relu\"`\n",
    "- `Init tip = \"Kaiming`\n",
    "- `Const val = 0.05`          \n",
    "- `# epoha = 30`              \n",
    "- `LR = 1e-3`                 \n",
    "- `Train BS = 150`            \n",
    "- `Valid ratio = 0.10`\n",
    "- `Max batch = 150`\n",
    "\n",
    "Mreža 2.:\n",
    "- `hidden_dim = 300`\n",
    "- `# slojeva = 20`\n",
    "- `Init scale = 1.0`         \n",
    "- `Aktivacija = \"relu\"`\n",
    "- `Init tip = \"Kaiming`\n",
    "- `Const val = 0.05`          \n",
    "- `# epoha = 30`              \n",
    "- `LR = 1e-3`                 \n",
    "- `Train BS = 150`            \n",
    "- `Valid ratio = 0.10`\n",
    "- `Max batch = 150`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9784bb3-3f8e-4712-af99-b23d9b30e672",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Učitavanje programskih knjižica\n",
    "from Skripte.Vjezba8.widget import deep_mlp_training_widget\n",
    "\n",
    "_ui = deep_mlp_training_widget()\n",
    "_ui"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
