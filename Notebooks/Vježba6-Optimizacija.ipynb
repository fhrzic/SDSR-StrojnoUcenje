{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "feaa10bc-1264-449b-b8cc-4b70bcfd64a7",
   "metadata": {},
   "source": [
    "# Optimizacija \n",
    "\n",
    "<center><img src=\"Images/V6_banner.png\" width=\"700\" height=\"700\"/></center>\n",
    "\n",
    "U drugoj vježbi pokazano je da se rješenje većine problema strojnog učenja može svesti na \n",
    "\n",
    "* Ulazne podatke \n",
    "\n",
    "* Odabir modela \n",
    "\n",
    "* Funkcija cilja\n",
    "\n",
    "* **Optimizacija modela**\n",
    "\n",
    "Optimizacija modela znači pronalazak odgovarajućih vrijednosti modela koje na odabranoj funkciji cilja polučuju najbolje rezultate. U ovoj vježbi postepeno će se izgraditi motivacija i ideja iz sljedećih alogirtama optimizacije koji kreću od jednostavnijeg ka kompleksnijem:\n",
    "\n",
    "1. Linearna pretraga\n",
    "2. Gradijentni spust\n",
    "3. Stohastički gradijentni spust\n",
    "4. Momentum\n",
    "5. ADAM\n",
    "\n",
    "---\n",
    "\n",
    "# Podsjetnik funkcije cilja\n",
    "\n",
    "Funkcija cilja može se predstaviti kao prostor u kojemu tražimo globalni ili dovoljno dobar lokalni minumum. \n",
    "\n",
    "| 2D Funkcija cilja | 3D funkcija cilja |\n",
    "|:-----------------:|:-----------------:|\n",
    "| ![2D](Images/V6_loss2D.png) | ![3D](Images/V6_loss3D.png) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4f4c1a-078a-4bd8-8812-ecc47ff93e0a",
   "metadata": {},
   "source": [
    "## Linearna pretraga\n",
    "\n",
    "Jedan od algoritama za pronalazak lokalnog, odnosno globalnog minimuma jest linearna pretraga koji se može raspisati ilustirati na sljedeći način:\n",
    "\n",
    "* Započnemo sa inervalom koji je definiran točkama $[a,d]$ takvim da $a < d$ a unutar kojih smo sigurni da se nalazi minimum.\n",
    "* Potom odaberimo dvije točke $b$ i $c$ unutar intervala takve da vrijedi $b < c$ i evaluirajmo funkciju gubitka u tim točkama odnosno $L(b)$ i $L(c)$. Tipično 1/3 i 2/3 intervala.\n",
    "* Ažurirajmo interval primjenjujući iduče pravilo:\n",
    "    * Ako je $L(b) > L(c)$, minimum je unutar intervala $[b,d]$ pa stoga ažuriramo $a \\rightarrow b$.\n",
    "    * Ako je $L(b) \\leq L(c)$, minimum je unutar intervala $[a,c]$ pa stoga ažuriramo $d \\rightarrow c$.\n",
    "* Ponavljamo postupak dok interval ne postane manje od prethodno zadane $tolerancije$\n",
    "\n",
    "Slikovni prikaz navedenog algoritma nalazi se na idućoj slici:\n",
    "\n",
    "<center><img src=\"Images/V6_linear.png\" width=\"500\" height=\"500\"/></center>\n",
    "\n",
    "Kao što se može vidjeti, interval se dijeli na tri ravnomjerna dijela (trisekcija)\n",
    "\n",
    "---\n",
    "\n",
    "<font color='red'>\n",
    "\n",
    "\n",
    "## Zadatak\n",
    "\n",
    "<left><img src=\"Images/Zadatak.png\" width=\"70\" height=\"70\"/></left>\n",
    "\n",
    "</font>\n",
    "\n",
    "U skripti **Vjezba6/linearna_pretraga.py** potrebno je implementirati funkciju **bracketing_step** koja računa prethodno opisani algoritam (bez ponavljanja i tolerancije). Potom izvršite idući programski kod i odgovorite na pitanja:\n",
    "\n",
    "* Konvergira li uvijek algoritam linearne pretrage?\n",
    "* Kako na algoritam utječe parametar broja iteracija, a kako tolerancije?\n",
    "* Pronađe li algoritam uvijek optimalno rješenje?\n",
    "* Što se dogodi ako je inicijalni interval $[a,d]$ krivo definiran, primjerice premali ili preveliki?\n",
    "* Kako odabrati dobar incijalni interval?\n",
    "* Izračunajte ručno idući korak intervala.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ca01c6-b084-48cd-b30a-268f21d59806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Pokretanje skripte \n",
    "from Skripte.Vjezba6.linearna_pretraga_rj import linear_search_widget\n",
    "\n",
    "linear_search_widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67344467-0550-4a1f-96bc-f2ef375b55cd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a59019-5570-44bb-9691-394f3faecc81",
   "metadata": {},
   "source": [
    "## Gradijentni spust\n",
    "\n",
    "U drugoj vježbi je uveden gradijentni spust koji se računa po sljedećoj formuli:\n",
    "\n",
    "$$\\Theta_{n+1} = \\Theta_n - \\alpha\\cdot\\bigtriangledown_\\Theta J(\\Theta) $$\n",
    "\n",
    "Odnosno derivacijom funkcije cilja po ulaznim parametrima modela. Dok gradijent -- točnije negativni gradijent ukazuje na smjer koji vodi ka optimalnom rješenju, dok stopa učenja $\\alpha$ kaže za koliko se se valja pomaknuti u tom smjeru u promatranom koraku.\n",
    "\n",
    "Za funkciju cilja $MSE$ i linearnu regresiju ($y = \\theta_0 + \\theta_1 \\cdot x$) gradijent se računa pomoću sljedeće formule:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial \\boldsymbol\\phi} = \\begin{bmatrix}\\frac{\\partial L}{\\partial \\phi_0} \\\\\\frac{\\partial L}{\\partial \\phi_1} \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Odnosno:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\ell_i}{\\partial \\boldsymbol{\\phi}}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial \\ell_i}{\\partial \\phi_0} \\\\\n",
    "\\frac{\\partial \\ell_i}{\\partial \\phi_1}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "2(\\phi_0 + \\phi_1 x_i - y_i) \\\\\n",
    "2x_i(\\phi_0 + \\phi_1 x_i - y_i)\n",
    "\\end{bmatrix}.\n",
    "\\tag{6.7}\n",
    "\\end{align}\n",
    "\n",
    "Također, gradijent se može računati i aproksimativno:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial \\phi_{0}}&\\approx & \\frac{L[\\phi_0+\\delta, \\phi_1]-L[\\phi_0, \\phi_1]}{\\delta}\\\\\n",
    "\\frac{\\partial L}{\\partial \\phi_{1}}&\\approx & \\frac{L[\\phi_0, \\phi_1+\\delta]-L[\\phi_0, \\phi_1]}{\\delta}\n",
    "\\end{align}\n",
    "\n",
    "gdje $\\delta$ označava neki jako mali broj poput $e^{-7}$.\n",
    "\n",
    "---\n",
    "\n",
    "<font color='red'>\n",
    "\n",
    "\n",
    "## Zadatak\n",
    "\n",
    "<left><img src=\"Images/Zadatak.png\" width=\"70\" height=\"70\"/></left>\n",
    "\n",
    "</font>\n",
    "\n",
    "U skripti **Vjezba6/gradijentni_spust.py**  implementirajte funkcije **gradijent_derivacija** i **gradijent_aproksimacija** te pokrenite idući programski kod te odgovorite na pitanja:\n",
    "\n",
    "* Koja je razlika u implmentaciji oba pristupa?\n",
    "* Polučuju li pristupi isto rješenje?\n",
    "* Derivirajte MSE funkciju na papiru i odredite ručno derivacije koje su prethodno za vas izračunate.\n",
    "* Kako utječe parametar delta na gradijentnu aproksimaciju?\n",
    "* Kako početna vrijednost parametra $\\Theta$ utječe na algoritme?\n",
    "* **X** Koje su mane gradijentnog spusta? O čemu on najviše ovisi te kako se te mane mogu prevladati? Uzmite u obzir funkcije cilja iz linearnog pretraživanja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2d770e-b407-41aa-a57f-f3bb019dc74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Knjižnice\n",
    "from Skripte.Vjezba6.gradijentni_spust_rj import *\n",
    "from Skripte.Vjezba3.dataloader import *\n",
    "import os\n",
    "   \n",
    "# Pokretanje skripte \n",
    "# Izgradimo dataset i dataloader\n",
    "_dataset = bikeRentalDataset(\n",
    "    path_to_csv=os.path.join(\"Skripte\",\"Vjezba3\", \"day_bikes_rental.csv\"),\n",
    "    input_label=\"temp\",\n",
    "    target_label=\"cnt\",\n",
    "    normalizacija=True,\n",
    ")\n",
    "_dataloader = ordinary_dataloader(dataset=_dataset, batch_size=1)\n",
    "\n",
    "_xs, _ys = [], []\n",
    "for _x_batch, _y_batch in _dataloader:\n",
    "    _xs.append(_x_batch.squeeze().item())\n",
    "    _ys.append(_y_batch.squeeze().item())\n",
    "_x = np.asarray(_xs, dtype=float).reshape(-1)\n",
    "_y = np.asarray(_ys, dtype=float).reshape(-1)\n",
    "\n",
    "# Postavimo thetu\n",
    "_theta = np.array([0.5,0.25])\n",
    "\n",
    "# Pozivi\n",
    "_g_a = gradijent_aproksimacija(y = _y, x = _x, theta = _theta, delta = 1e-7)\n",
    "_g_d = gradijent_derivacija(y = _y, x = _x, theta = _theta)\n",
    "\n",
    "# Ispis (Trebaju bti približno isti: [0.24042378 0.09295978], [0.24042388 0.09295981]\n",
    "print(f\"Gradijenti Theta računati pomoću derivacije: {_g_d}\\nGradijenti Theta računati pomoću aproksimacije: {_g_a}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16902497-e5f5-425d-86af-0e17bcd3e941",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1464f332-c986-4d9c-80a8-d8b03fb6d9c6",
   "metadata": {},
   "source": [
    "<font color='red'>\n",
    "\n",
    "\n",
    "## Zadatak\n",
    "\n",
    "<left><img src=\"Images/Zadatak.png\" width=\"70\" height=\"70\"/></left>\n",
    "\n",
    "</font>\n",
    "\n",
    "Sljedeći programski kod omogućuje pomocanje $\\Theta$ parametara kao i promatranje konvergencije/izračuna gradijentnog spusta. Pokretanjem koda i mijenjanjem parametara odgovorite na iduća pitanja:\n",
    "\n",
    "* Kako utječe parametar $\\alpha$ na gradijentni spust, a kako inicjalna vrijednost parametra $\\Theta$?\n",
    "* Što se dogodi kada je $\\alpha$ jako mali a broj iteracije postavljen na maksimum? ($\\Theta$ parametar je maksimalno negativan.)\n",
    "* Što se dogodi kada je broj iteracija mali (primjerice 5) a $\\alpha$ postavljen na 0.01? ($\\Theta$ parametar je maksimalno negativan.)\n",
    "* Može li gradijenti spust divergirati, ako da, u kojem scenariju (nacrtajte)?.\n",
    "* Koja je uloga broja iteracija?\n",
    "* Trenutna implementacija koristi fiksnu veličinu parametra $\\alpha$ između dva koraka gradijentnog spusta. Predložite način na koji se parametar $\\alpha$ adaptira iz koraka u korak. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f13ed9-a835-4c02-bb19-448d4adf4200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Pokretanje skripte \n",
    "from Skripte.Vjezba6.gradijentni_spust_rj import *\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "plot_bike_side_by_side()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3e2b77-aee2-4189-b8f5-a28a99705855",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc6e416-71d1-4a1a-80fc-126c39cec64d",
   "metadata": {},
   "source": [
    "## Stohastički gradijentni spust\n",
    "\n",
    "Jedan od glavnih problema gradijentnog spusta je taj što je konačno odredište algoritma u potpunosti određeno početnom točkom. **Stohastički gradijentni spust (SGD)** pokušava riješiti ovaj problem dodavanjem malo šuma gradijentu u svakom koraku. Rješenje se i dalje u prosjeku kreće nizbrdo, ali u bilo kojoj iteraciji, odabrani smjer nije\n",
    "nužno u najstrmijem smjeru nizbrdo. Zapravo, možda uopće nije nizbrdo. SGD algoritam ima mogućnost privremenog kretanja uzbrdo i time skakanja iz jedne \"doline\" funkcije gubitka u drugu. \n",
    "\n",
    "Mehanizam uvođenja slučajnosti je jednostavan. U svakoj iteraciji algoritam odabire nasumičan podskup trening skupa i računa gradijent koristeći samo te primjere. Ovaj podskup naziva se **minibatch** ili **jednostavno batch.**\n",
    "\n",
    "Pravilo ažuriranja za parametre modela:\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\phi_{t+1} \\leftarrow \\phi_t - \\alpha \\cdot \\sum_{i \\in B_t} \\frac{\\partial \\ell_i[\\phi_t]}{\\partial \\phi}\n",
    "\\end{align}\n",
    "\n",
    "gdje je $B_t$ skup indeksa ulazno/izlaznih parova u trenutnom batchu, a kao i prije $\\ell_i$ je gubitak za $i$-ti par. Parametar  $\\alpha$ predstavlja learning rate (korak učenja) i zajedno s veličinom gradijenta određuje veličinu pomaka u svakom koraku učenja. Vrijednost learning rate-a određuje se na početku treniranja i ne ovisi o lokalnim svojstvima funkcije gubitka.\n",
    "\n",
    "Batch-evi se obično uzimaju iz skupa podataka bez ponavljanja. Algoritam prolazi kroz sve primjere sve dok ne prođe kroz cijeli skup, nakon čega ponovno počinje od početka. Jedan prolazak kroz cijeli skup podataka naziva se epoch.\n",
    "\n",
    "Veličina batcha može varirati:\n",
    "\n",
    "ako batch ima samo jedan primjer → stohastički gradijentni spust (SGD),\n",
    "\n",
    "ako batch sadrži cijeli trening skup → full-batch gradijentni spust, koji je jednak standardnom (determinističkom) gradijentnom spustu.\n",
    "\n",
    "Manji batch-i uvode slučajnost u proces učenja, što može pomoći algoritmu da izbjegne lokalne minimume te ubrzati učenje na velikim skupovima podataka.\n",
    "\n",
    "---\n",
    "\n",
    "<font color='red'>\n",
    "\n",
    "\n",
    "## Zadatak\n",
    "\n",
    "<left><img src=\"Images/Zadatak.png\" width=\"70\" height=\"70\"/></left>\n",
    "\n",
    "</font>\n",
    "\n",
    "U skripti **Vjezba6/sgd.py** implementirajte u funkciji **sgd** stohastički gradijentni spust. Zatim pokrenite idući programski kod i odgovorite na iduća pitanja:\n",
    "\n",
    "* Koje razlike uočavate u putanjama koje ilustriraju konvergenciju SGD-a i GD-a?\n",
    "* Kako pormjene svakog od parametara utječu na ponašanje optimizacijske funkcije?\n",
    "* Ako zamislite funkcije cilja koje su predstavljenje u linearnom pretraživanju, kako će se na njima ponašati GD a kako SGD? Što očekujete od SGD-a?\n",
    "* Komentirajte dodatno batch_size? Za koji batch_size SGD postaje GD?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db963308-8fb6-48cd-b0da-83fa3711e9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Pokretanje skripte \n",
    "from Skripte.Vjezba6.sgd_rj import *\n",
    "%matplotlib widget\n",
    "sgd_widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec74f117-6b49-41b1-91ba-0e66057fca3f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a99ef86-5ad9-4373-b5a7-11f0011e3870",
   "metadata": {},
   "source": [
    "## Momentum\n",
    "\n",
    "**Momentum** je uobičajena nadogradnja stohastičkog gradijentnog spusta (SGD). Ideja je ažurirati parametre koristeći ponderiranu kombinaciju gradijenta iz tekućeg batcha i smjera kretanja u prethodnom koraku:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{m}_{t+1}\n",
    "\\leftarrow\n",
    "\\beta \\cdot \\mathbf{m}_t\n",
    "+\n",
    "(1 - \\beta)\n",
    "\\sum_{i \\in B_t}\n",
    "\\frac{\\partial \\ell_i(\\boldsymbol{\\phi}_t)}{\\partial \\boldsymbol{\\phi}}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\boldsymbol{\\phi}_{t+1}\n",
    "\\leftarrow\n",
    "\\boldsymbol{\\phi}_t\n",
    "-\n",
    "\\alpha \\cdot \\mathbf{m}_{t+1}\n",
    "\\end{align}\n",
    "\n",
    "gdje $m_t$ predstavlja momentum u koraku $t$ (akumulirani smjer kretanja), $\\beta in[0,1)$ određuje kolko se momentum \"izglađuje\" kroz vrijeme, $\\alpha$ je stopa učenja, $B_t$ je minibatch podataka u iteraciji \n",
    "$t$. U konačnici parametri $\\Theta$ se ažuriraju korištenjem momentuma, čime se:\n",
    "\n",
    "* ubrzava kretanje u dosljednim smjerovima gradijenta,\n",
    "* prigušuju oscilacije u uskim dolinama (što se često javlja u optimizaciji),\n",
    "* putanja optimizacije postaje glatkija i stabilnija, posebno u blizini minimuma.\n",
    "---\n",
    "\n",
    "## Nesterov momentum:\n",
    "\n",
    "Nesterov momentum uvodi malu promjenu u kojoj:\n",
    "\n",
    "* Prvo napravi mali korak u smjeru trenutnog momentuma, kao predikciju\n",
    "* Zatim izračunaj gradijent u toj predviđenoj točki (umjesto u staroj točki)\n",
    "\n",
    "Ova preinaka omogućuje bolju kontrolu i manje oscilacija kada se krivulja približava minimumu, što ujedno dovodi do manje oscilacija i brže konvergencije.\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{m}_{t+1}\n",
    "\\leftarrow\n",
    "\\beta\\cdot \\mathbf{m}_t\n",
    "+\n",
    "(1-\\beta)\n",
    "\\sum_{i\\in B_t}\n",
    "\\frac{\\partial \\ell_i\\!\\left(\\boldsymbol{\\phi}_t - \\alpha\\beta \\cdot \\mathbf{m}_t\\right)}{\\partial \\boldsymbol{\\phi}}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\boldsymbol{\\phi}_{t+1}\n",
    "\\leftarrow\n",
    "\\boldsymbol{\\phi}_t\n",
    "-\n",
    "\\alpha \\cdot \\mathbf{m}_{t+1}\n",
    "\\end{align}\n",
    "\n",
    "---\n",
    "\n",
    "<font color='red'>\n",
    "\n",
    "\n",
    "## Zadatak\n",
    "\n",
    "<left><img src=\"Images/Zadatak.png\" width=\"70\" height=\"70\"/></left>\n",
    "\n",
    "</font>\n",
    "\n",
    "* Pokrenite idući kod koji implementira momentum i iznesite zaključke o ulozi parametara i ponašanju Nesterov Momentm optimizatora.\n",
    "* Dodatno proučite i pronađite više informacija o ADAM i ADAMW optimizatoru. Koje su njihove prednosti nad prikazanim optimizatorima. \n",
    "* Što je to learning rate scheduler?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a2634d-bad3-4910-8cc4-ab0fb981fed4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Pokretanje skripte \n",
    "from Skripte.Vjezba6.momentum import *\n",
    "%matplotlib widget\n",
    "sgd_widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4ec634-602b-461a-a1ec-c58d1e1884fc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30e8a94-2f4f-4790-ba9b-3c6f565f43a6",
   "metadata": {},
   "source": [
    "## ADAM\n",
    "\n",
    "**Problemi sa dosadašnjim pristupom**: Gradientni spust s fiksnim korakom ima sljedeće nepoželjno svojstvo: algoritam generira/radi velike prilagodbe parametara povezanih s velikim gradijentima (gdje bi se trebalo biti opreznim) i male prilagodbe parametara povezanih s malim gradijentima (gdje bi trebali istražiti više). Kada je gradijent funkcije gubitka mnogo strmiji u jednom smjeru nego u drugom (n-D, u 1D prostoru ovog problema nema, ali 1D prostor znači i samo jedan parametar mdoela), teško je odabrati faktor učenja koji (i) omogućuje dobar napredak u oba smjera i (ii) ostaje stabilan. \n",
    "\n",
    "Jednostavan pristup je normalizirati gradijente tako da se gradijent minimizira fiksnom udaljenošću (određenom faktorom učenja) u svakom smjeru. Da bi se to učinilo, prvo se mjeri gradijent $m_{t+1}$ i kvadrirani gradijent gradijent $v_{t+1}$\n",
    "\\begin{align}\n",
    "\\mathbf{m}_{t+1} \\leftarrow \\frac{\\partial L(\\phi_t)}{\\partial \\phi}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{v}_{t+1} \\leftarrow \\left( \\frac{\\partial L(\\phi_t)}{\\partial \\phi} \\right)^2\n",
    "\\end{align}\n",
    "\n",
    "Potom se primjenjuje pravilo ažuriranja sa normaliziranim gradijentom:\n",
    "\n",
    "\\begin{align}\n",
    "\\phi_{t+1} \\leftarrow \\phi_t - \\alpha \\cdot \\frac{\\mathbf{m}_{t+1}}{\\sqrt{\\mathbf{v}_{t+1}} + \\epsilon}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "gdje je $\\alpha$ stopa učenja, $\\epsilon$ mala konstanta koja sprječava djeljenje s nulom kada je gradijent jednak nuli. Član $v_{t+1}$ predstavlja kvadrirani gradijent, a njegov pozitivni korjen služi za normaliziranje samog gradijenta. \n",
    "\n",
    "Ovaj jednostavan algoritam omogućuje napredak u oba smjera, ali neće konvergirati osim ako slučajno ne dospije točno u minimum. Umjesto toga, poskakivat će naprijed-natrag oko minimuma. Adaptivna procjena momenta, odnosno **Adam**, preuzima ovu ideju i dodaje moment i na procjenu gradijenta i na kvadrirani gradijent.\n",
    "\n",
    "Odnosno **Adaptive Moment Estimation Adam** je definiran pomoću pristupa u kojem se zadržava eksponencijalna pomična sredina i gradijenta i njihovih kvadrata:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{m}_{t+1} \\leftarrow \\beta \\cdot \\mathbf{m}_t + (1 - \\beta)\\frac{\\partial L(\\phi_t)}{\\partial \\phi}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{v}_{t+1} \\leftarrow \\gamma \\cdot \\mathbf{v}_t + (1 - \\gamma)\\left( \\frac{\\partial L(\\phi_t)}{\\partial \\phi} \\right)^2\n",
    "\\end{align}\n",
    "\n",
    "gdje su $\\beta$ i $\\gamma$ koeficijenti momenta za ove dvije statistike.\n",
    "\n",
    "Korištenje momenta ekvivalentno je uzimanju ponderiranog prosjeka kroz povijest ovih statistika. Na početku su svi prethodni gradijenti blizu nule, što bi dovelo do nerealno malih procjena. Zato uvodimo korekciju pristranosti:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\mathbf{m}}_{t+1} \\leftarrow \\frac{\\mathbf{m}_{t+1}}{1 - \\beta^{t+1}}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\mathbf{v}}_{t+1} \\leftarrow \\frac{\\mathbf{v}_{t+1}}{1 - \\gamma^{t+1}}\n",
    "\\end{align}\n",
    "\n",
    "Budući da su $\\beta$ i $\\gamma$ u rasponu $[0, 1]$, članovi $\\beta^{t+1}$ i $\\gamma^{t+1}$ postaju sve manji kako vrijeme teče pa nazivnici postaju bliski 1, odnosno učinak korekcija slabi kako se optimizacija nastavlja. \n",
    "\n",
    "Samo ažuriranje gradijenta glasi:\n",
    "\n",
    "\\begin{align}\n",
    "\\phi_{t+1} \\leftarrow \\phi_t - \\alpha \\cdot \\frac{\\hat{\\mathbf{m}}_{t+1}}{\\sqrt{\\hat{\\mathbf{v}}_{t+1}} + \\epsilon}\n",
    "\\end{align}\n",
    "\n",
    "Naravno, Adam se koristi i u stohastičkom okruženju kada se optimizacija vrši u mini-batchevima. Posljedično, formule se ažuriraju u njihovu stohastičku verziju:\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{m}_{t+1} \\leftarrow \\beta \\cdot \\mathbf{m}_t + (1 - \\beta)\\left(\\sum_{i \\in \\beta_t} \\frac{\\partial l_i(\\phi_t)}{\\partial \\phi} \\right)\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{v}_{t+1} \\leftarrow \\gamma \\cdot \\mathbf{v}_t + (1 - \\gamma)\\left(\\sum_{i \\in \\beta_t} \\left( \\frac{\\partial l_i(\\phi_t)}{\\partial \\phi} \\right)^2 \\right)\n",
    "\\end{align}\n",
    "\n",
    "---\n",
    "\n",
    "<font color='red'>\n",
    "\n",
    "\n",
    "## Zadatak\n",
    "\n",
    "<left><img src=\"Images/Zadatak.png\" width=\"70\" height=\"70\"/></left>\n",
    "\n",
    "</font>\n",
    "\n",
    "Procjenite kakvu će trajektoriju imati ADAM algoritam. Skicirajte istu za proizvoljni primjer na papiru.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
