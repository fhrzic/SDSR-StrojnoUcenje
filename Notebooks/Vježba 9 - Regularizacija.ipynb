{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d2e30d0-c68c-47fd-b44d-4d10e0bcbb62",
   "metadata": {},
   "source": [
    "# Regularizacija parametara modela \n",
    "\n",
    "\n",
    "<center><img src=\"Images/V9_banner.png\" width=\"700\" height=\"700\"/></center>\n",
    "\n",
    "# Uvod u regularizaciju\n",
    "\n",
    "Regularizacija je skup tehnika kojima se sprječava **prenaučenost (overfitting)** i poboljšava **generalizacija** modela na neviđene podatke. Tijekom treniranja neuronskih mreža model uči parametre koji minimiziraju funkciju gubitka na skupu za učenje. Međutim, ako je model prekomjerno složen ili ako imamo premalo podataka, mreža može “zapamtiti” trening primjere umjesto da nauči opća pravila. Tada dobivamo vrlo nisku pogrešku na trening skupu, ali visoku pogrešku na test skupu.\n",
    "\n",
    "Regularizacija uvodi **dodatna ograničenja** ili **penale** na parametre modela kako bi se smanjila mogućnost prekomjernog prilagođavanja šumu u podacima. Time se model usmjerava prema jednostavnijim i stabilnijim rješenjima koja bolje generaliziraju.\n",
    "\n",
    "Najčešće korištene metode regularizacije uključuju:\n",
    "\n",
    "- **L2 regularizacija (Ridge)** – kažnjava velika težinska rješenja i potiče glatke modele.  \n",
    "- **L1 regularizacija (Lasso)** – potiče rijetkost (sparse) parametara i može eliminirati nepotrebne značajke.  \n",
    "- **Dropout** – nasumično “isključuje” neurone tijekom treniranja, čime sprječava njihovu međusobnu ovisnost.  \n",
    "- **Early stopping** – prekida treniranje kada se performanse na validacijskom skupu počnu pogoršavati.  \n",
    "- **Data augmentation** – povećava količinu i raznolikost podataka kako bi model vidio više varijacija.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb60ecb-5cd7-492c-8310-a2233af33727",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## L1 (Lasso) regularizacija\n",
    "\n",
    "L1 regularizacija dodaje apsolutne vrijednosti težina u funkciju gubitka:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L}_{\\text{L1}}(\\mathbf{w}) = \\mathcal{L}_{\\text{data}}(\\mathbf{w}) + \\lambda \\sum_{j=1}^{d} |w_j|\n",
    "\\end{align}\n",
    "\n",
    "L1 potiče **rijetkost (sparsity)** — mnoge težine postaju točno nula.\n",
    "\n",
    "---\n",
    "\n",
    "## L2 (Ridge) regularizacija\n",
    "\n",
    "L2 regularizacija dodaje kvadrate težina:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L}_{\\text{L2}}(\\mathbf{w}) = \\mathcal{L}_{\\text{data}}(\\mathbf{w}) + \\lambda \\sum_{j=1}^{d} w_j^2\n",
    "\\end{align}\n",
    "\n",
    "L2 potiče **male, glatke vrijednosti težina**, ali rijetko ih postavlja točno na nulu.\n",
    "\n",
    "---\n",
    "\n",
    "<font color='green'>\n",
    "\n",
    "\n",
    "## Primjer\n",
    "\n",
    "<left><img src=\"Images/Primjer.png\" width=\"70\" height=\"70\"/></left>\n",
    "</font>\n",
    "\n",
    "Slijedeća dvije neuronske mreže polučuju isti rezultat -- istu predikciju ($0.2$), ali se bitno razlikuju u težinama $\\Theta$:\n",
    "\n",
    "<center><img src=\"Images/V9_regularizacija.png\" width=\"700\" height=\"700\"/></center>\n",
    "\n",
    "Neuronska mreža a) ima redom težine $\\Theta = [0, 1, 0, 0, 0]$ dok neruonska mreža b) ima težine $\\Theta = [-0.03, 0.4, 0.1, 0.2, 0.3]$. Ako se razmilsi, u slučaju neuronske mreže a) ukoliko neuorn sa težinom $1$ iz bilo razloga se ne aktivira, pogreška će automatski biti kriva jer je praktički cijela \"odgovornost\" postavljena na jedna neuron. S druge strane, neuronska mreža b) u slučaju zatajenja jednog neuorona i dalje ima ostale neurone koji doprinose ukupnoj odluci. Dakle cilj je distribuirati težine kako jedan neuron ne bi imao preveliki utjecaj.\n",
    "\n",
    "U izrazima za $\\text{L1}$ i $\\text{L2}$ regularizaciju, s obzirom da obje neuronske mreže daju istu predikciju $0.2$, $\\mathcal{L}$ će biti isti, dok $\\lambda$ skalira dodatni član jednadžbe koji potiče i uvjetuje oblik težina. Pručimo za obje regularizacije koliko ovi članovi iznose (radi primjera $\\lambda = 1$):\n",
    "\n",
    "Za neuronsku mrežu a), L1: $\\lambda \\sum_{j=1}^{d} |w_j| = 0+1+0+0+0 = 1$, L2:$\\lambda \\sum_{j=1}^{d} w_j^2 = 1$\n",
    "\n",
    "Za neuronsku mrežu b), L1: $\\lambda \\sum_{j=1}^{d} |w_j| = 0.03 + 0.4 + 0.1 + 0.2 + 0.3 = 1.13$, L2: $\\lambda \\sum_{j=1}^{d} w_j^2 = 0.0009+0.16+0.01+0.04+0.09=0.3009$ \n",
    "\n",
    "Usporedbom uočavamo da L1 norma faovrizira da jedna težina nosi veću odgovornost, dok L2 norma nastoji raspodijeliti odgovornosti na više težina. Sjetite se, cilj je minimizirati funkciju cilja stoga što manji iznost daje element, to je zapravo rješenje bolje.\n",
    "\n",
    "---\n",
    "\n",
    "## Dropout regularzacija\n",
    "\n",
    "\n",
    "Intuicija iza dropouta je vrlo jednostavna: tijekom svake iteracije treniranja **nasumično se izbace (postave na 0)** dio neurona u mreži ([Poveznica](https://jmlr.org/papers/v15/srivastava14a.html)).\n",
    "\n",
    "To znači:\n",
    "\n",
    "* svaki **batch** vidi **drugačiju “stanjenu” (thinned) mrežu**,\n",
    "* mreža se uči ponašati **robustno**, bez oslanjanja na to da su baš svi neuroni prisutni,\n",
    "* efekt je sličan kao da treniramo **veliki ansambl** mnogo manjih mreža koje **dijele iste parametre**\n",
    "\n",
    "Na taj način:\n",
    "\n",
    "* smanjuje se šansu da se neuroni **previše “dogovaraju”** (co-adaptation),  \n",
    "* poboljšava se **generalizacija** na novim podacima.\n",
    "\n",
    "-- \n",
    "Jedan skriveni sloj ima sljdeće aktivacije\n",
    "\n",
    "$\n",
    "\\mathbf{h} = f(\\mathbf{z})\n",
    "$  \n",
    "gdje je $(f)$ neka nelinearna aktivacija (ReLU, tanh, …).\n",
    "\n",
    "Postupak je sljdeći:\n",
    "\n",
    "1. **Generira se maska** slučajnih nula i jedinica:\n",
    "   $\n",
    "   m_i \\sim \\text{Bernoulli}(1-p)\n",
    "   $\n",
    "\n",
    "2. **Primijeni se maska na aktivacije**:\n",
    "   $\n",
    "   \\tilde{\\mathbf{h}} = \\mathbf{m} \\odot \\mathbf{h}\n",
    "   $\n",
    "\n",
    "3. **Nastavlja se propagacija** koristeći $(\\tilde{\\mathbf{h}})$.\n",
    "\n",
    "Tijekom **svakog batcha** maska $(\\mathbf{m})$ se ponovno nasumično generira.\n",
    "Svaki neuron je **aktiviran** s vjerojatnošću $(1-p)$, a **gašen** s vjerojatnošću $(p)$.\n",
    "\n",
    "Bitno je za raspraviti što se događa tijekom inferencije sa dropout slojem.\n",
    "Postoje dva pristupa:\n",
    "\n",
    "**1. Standardni dropout (originalni rad):**\n",
    "* Tijekom treniranja: gasimo dio neurona.\n",
    "* Tijekom testa: **ne gasimo neurone**, ali skaliramo težine s $(1-p)$.\n",
    "\n",
    "**2. Inverted dropout (moderna implementacija):**\n",
    "* Tijekom treniranja:\n",
    "  $\n",
    "  \\tilde{h}_i = \\frac{m_i \\cdot h_i}{1-p}\n",
    "  $\n",
    "* Tijekom testa: ne radi se ništa posebno, niti skaliramo niti gasimo dropout.\n",
    "\n",
    "U praksi (PyTorch, TensorFlow) koristi se **inverted dropout**.\n",
    "\n",
    "---\n",
    "### Prednosti i nedostaci\n",
    "\n",
    "**Prednosti:**\n",
    "- jednostavan za implementirati  \n",
    "- učinkovita regularizacija  \n",
    "- radi kao implicitni ensemble  \n",
    "- dobro se kombinira s L2, augmentacijom, batch normom (ovisno o arhitekturi)\n",
    "\n",
    "**Nedostaci:**\n",
    "- može usporiti treniranje (potrebno više epoha)  \n",
    "- nije uvijek najbolji za moderne arhitekture (npr. veliki Transformeri)  \n",
    "- potrebno je prilagoditi hiperparametar \\(p\\)\n",
    "\n",
    "---\n",
    "\n",
    "## Augmentacija podataka\n",
    "\n",
    "Augmentacija podataka je skup tehnika kojima se **umjetno povećava trenirni skup** tako da se postojeće slike ili uzorci transformiraju na način koji **zadržava bitnu semantiku**, ali uvodi dodatnu varijabilnost.\n",
    "\n",
    "Ključni razlozi zašto se koristi:\n",
    "\n",
    "* **Smanjuje overfitting** – model vidi više varijacija i ne uči samo “napamet”.\n",
    "* **Povećava generalizaciju** – bolje performanse na testnim podacima.\n",
    "* **Pomaže modelu postati robusniji** na rotacije, pomake, šumove, promjene osvjetljenja, perspektive itd.\n",
    "* **Umjetno povećava dataset** kada je broj uzoraka mali (čest slučaj u medicinskim podacima).\n",
    "* U nekim slučajevima poboljšava **invarianciju** modela (npr. translacijska invarijancija kod CNN-ova).\n",
    "\n",
    "Augmentacija se gotovo uvijek koristi **samo tijekom treniranja**, a ne tijekom testiranja.\n",
    "\n",
    "---\n",
    "Osnovna ideja augmentacije je vrlo jednostavna: Primijenjuju se nasumične transformacije na ulazne podatke svaki put kad ih model vidi, kako bi isti uzorak svaki put izgledao malo drugačije. Na taj način model nauči:\n",
    "\n",
    "* ignorirati irelevantne varijacije,\n",
    "* fokusirati se na bitne karakteristike,  \n",
    "* postati robusniji na realne uvjete izvan trenirnog skupa.\n",
    "\n",
    "---\n",
    "\n",
    "Najčešće korištene augmentacije: \n",
    "\n",
    "#### 1. Geometrijske transformacije\n",
    "\n",
    "##### **Random rotacije**\n",
    "* Slike se rotiraju za neki slučajni kut (npr. ±15°).\n",
    "* Pomaže kod orijentacijskih varijacija.\n",
    "\n",
    "##### **Random translacije (pomaci)**\n",
    "* Slika se pomakne po x ili y osi.\n",
    "* Pomaže kod translacijske invariancije.\n",
    "\n",
    "##### **Random skaliranje (zoom in/out)**\n",
    "* Slika se slučajno približi ili udalji.\n",
    "* Pomaže CNN-u da se ne oslanja na apsolutnu veličinu objekta.\n",
    "\n",
    "##### **Random horizontal flip**\n",
    "* Najjednostavnija augmentacija.\n",
    "* Koristi se za prirodne slike; oprez kod medicinskih slika.\n",
    "\n",
    "##### **Random vertical flip**\n",
    "* Također korisno, ali pazi kod anatomije (npr. nije validno za CT glave).\n",
    "\n",
    "##### **Random cropping**\n",
    "* Isječe se nasumičan dio slike.\n",
    "* Uči model robusnosti na pozicioniranje i scale.\n",
    "\n",
    "##### **Random perspective / affine transformacije**\n",
    "* Blage promjene perspektive.\n",
    "* Korisno u objektnoj detekciji i klasifikaciji.\n",
    "\n",
    "#### 2. Fotometrijske transformacije\n",
    "\n",
    "##### **Promjene osvjetljenja**\n",
    "* Brightness, contrast, gamma.\n",
    "* Simulira različite uvjete snimanja.\n",
    "\n",
    "##### **Promjene zasićenja i nijanse**\n",
    "* Koristi se kod RGB slika, manje u medicinskim datasetima.\n",
    "\n",
    "##### **Dodavanje šuma**\n",
    "* Gaussian noise.\n",
    "* Model postaje robusniji na “realne” smetnje u podacima.\n",
    "\n",
    "##### **Gaussian blur**\n",
    "* Koristi se umjereno; model se uči nositi s mutnim podacima.\n",
    "\n",
    "##### **Color jitter**\n",
    "* Kombinira brightness/contrast/saturation/hue promjene.\n",
    "\n",
    "#### 3. Prostorne i strukturalne augmentacije\n",
    "\n",
    "##### **Cutout / Random Erasing**\n",
    "* Nasumično se “izbriše” mali kvadrat slike.\n",
    "* Sprječava model da se oslanja na jako male lokalne feature-e.\n",
    "\n",
    "##### **Mixup**\n",
    "* Kombinira dvije slike i labele:  \n",
    "  $\n",
    "  \\tilde{x} = \\lambda x_1 + (1-\\lambda) x_2\n",
    "  \\]\n",
    "  \\[\n",
    "  \\tilde{y} = \\lambda y_1 + (1-\\lambda) y_2\n",
    "  $\n",
    "* Povećava linearizira granice odlučivanja.\n",
    "\n",
    "##### **CutMix**\n",
    "* Izreže se patch iz jedne slike i zalijepi na drugu.\n",
    "* Kombinira prednosti cutout-a i mixup-a.\n",
    "\n",
    "##### **Random elastic deformation**\n",
    "* Koristi se često u medicinskoj obradi slike (npr. MRI/CT).\n",
    "* Simulira anatomske deformacije.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Specifične augmentacije za medicinske slike\n",
    "\n",
    "* **Histogram equalization / CLAHE**  \n",
    "  poboljšava kontrast kod slabo osvijetljenih skenova.\n",
    "\n",
    "* **Random intensity shifts**  \n",
    "  posebno korisno kod CT-a (promjene HU vrijednosti).\n",
    "\n",
    "* **Random gamma correction**  \n",
    "  prilagodba dinamike osvjetljenja.\n",
    "\n",
    "* **Elastic deformation**  \n",
    "  simulira anatomske varijacije.\n",
    "\n",
    "* **Spatial dropout / channel dropout**  \n",
    "  posebno za 3D CNN arhitekture.\n",
    "\n",
    "---\n",
    "\n",
    "### Kada augmentacija može biti loša?\n",
    "\n",
    "* Ako stvara **nerealistične primjere** (npr. flipping kod anatomije glave).  \n",
    "* Ako transformacije naruše **semantiku** (npr. prevelika rotacija kod X-ray slika može promijeniti smjer anatomije).  \n",
    "* Ako se previše agresivno augmentira → model \"nauči\" na nenormalnim primjerima.\n",
    "\n",
    "Pravila:\n",
    "\n",
    "* Geometrijske augmentacije: koristiti umjereno.  \n",
    "* Fotometrijske augmentacije: koristiti ovisno o domenskom znanju.  \n",
    "* U medicini: uvijek se zapitati **“može li ovakva slika postojati u stvarnom svijetu?”**.\n",
    "\n",
    "---\n",
    "\n",
    "## Transfer Learning\n",
    "\n",
    "Treniranje dubokih neuronskih mreža od nule zahtijeva:\n",
    "\n",
    "* **velike količine podataka**\n",
    "* **veliku računalnu snagu**\n",
    "* **dugo vrijeme treniranja**\n",
    "\n",
    "U praksi vrlo često **nemamo dovoljno podataka**, osobito u medicini, malim domenama ili specifičnim klasifikacijskim zadacima.  \n",
    "Zbog toga koristimo **transfer learning**, tehniku u kojoj “posuđujemo” znanje naučeno na velikim skupovima podataka.\n",
    "\n",
    "**Transfer learning** je pristup u kojem se model prvo trenira (ili uzmemo već istreniran model) na **velikom, općem datasetu**, a zatim ga **prilagođavamo novom, manjem i specifičnom problemu**. Tipičan primjer jest neuoronska mreža trenirana na ImageNet-u (1.2M slika, 1000 klasa) nauči univerzalne značajke (edge, texture, shapes). Te značajke mogu se **prijenijeti** na medicinske slike, prijenos domene, male datasete, i sl.\n",
    "\n",
    "### Prednosti transfer learninga\n",
    "\n",
    "* **brže treniranje** (model kreće od “pametnog” začetnog stanja)\n",
    "* **bolja generalizacija** na malim datasetima\n",
    "* **manji rizik od overfittinga**\n",
    "* često omogućuje **state-of-the-art rezultate** uz minimalno treniranje\n",
    "* u medicini često **nužan** zbog malih datasetova\n",
    "\n",
    "\n",
    "### Tri glavna načina transfer learninga\n",
    "\n",
    "##### 1. **Feature Extraction (zamrzavanje cijele mreže)**\n",
    "- Zamrznemo sve slojeve osim zadnjeg.\n",
    "- Model koristi već naučene feature mapove.\n",
    "- Brzo, stabilno; dobro za **male datasete**.\n",
    "\n",
    "##### 2. **Fine-Tuning (otključamo samo dio mreže)**\n",
    "- Prvo zamrznemo sve slojeve, treniramo samo zadnji sloj.\n",
    "- Zatim otključamo više slojeva i treniramo s malim LR.\n",
    "- Dobro kada imamo **srednje velik dataset**.\n",
    "\n",
    "##### 3. **Full Fine-Tuning**\n",
    "- Svi slojevi se treniraju.\n",
    "- Koristi se kada postoji **dovoljno podataka** ili kada je domena slična (npr. CT → CT).\n",
    "\n",
    "---\n",
    "Transfer larning daje benefite jer veliki modeli uče: rubove, teksture, oblike, kompozicijske strukture...\n",
    "Ove karakteristike su **općenite** i korisne za mnoge zadatke. Npr. CNN koji nauči prepoznati rubove na ImageNet-u, može te iste značajke koristiti i na medicinskim X-ray ili CT slikama. Time se izbjega treniranje od nule i koristi se “već naučena vizualnu abecedu”.\n",
    "\n",
    "Transfer learning s druge strane neće pomoći u slučaju kada:\n",
    "\n",
    "* Domene su previše različite:  \n",
    "  npr. ImageNet (RGB životinje i objekti) → MRI volumeni (grayscale, 3D).  \n",
    "  Ipak, čak i tada često daje bolje rezultate nego random inicijalizacija.\n",
    "\n",
    "* Posjedujemo **iznimno veliki skup podataka** -- bolje je trenirati od nule.\n",
    "\n",
    "* Ako su ulazni podaci vrlo specifični (npr. multispektralne satelitske slike), generalne značajke možda nisu idealne.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040d7ff3-89ea-4458-8631-384cfcd52a46",
   "metadata": {},
   "source": [
    "<font color='red'>\n",
    "\n",
    "\n",
    "## Zadatak\n",
    "\n",
    "<left><img src=\"Images/Zadatak.png\" width=\"70\" height=\"70\"/></left>\n",
    "</font>\n",
    "\n",
    "Idući programski kod omogućava istraživanje većine navedenih koncepata. Glavne značajke skupa podataka su:\n",
    "\n",
    "* Rad na MNIST skupu poadatak,\n",
    "* Definiranje dubinu i širinu mreže,\n",
    "* Tip i jačinu ($\\lambda$) L1/L2 regularizacije i dropout regularizacije,\n",
    "* Inspekcija norme gradijenata (nestajanje/eksplodiranje) kroz epohe,\n",
    "* Crtatnje box-plot distribucije aktivacija i gradijenata po izabranom sloju i epohama,\n",
    "* Pratiti macro F1 na validacijskom skupu kroz epohe te jednu završnu točku macro F1 na test skupu.\n",
    "\n",
    "Vaš je zadatak proučiti utjecaj navedenih regularizacije i hiperparametara mreže na konačne rezultate predikcija i same gradijente. Usporedite gradijente i preciznosti modela za modele u kojima se mijenjaju samo navedeni parametri (ista topologija, stopa učenja itd.):\n",
    "* L1 sa $\\lambda = 0.001$,\n",
    "* L2 sa $\\lambda 0.01$,\n",
    "* dropout sa 0.15,\n",
    "* Neuronska mreža bez regularzicaije\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "036b0fcc-6688-4aae-8aa5-c14ed8cab31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b76325d1acb14491ac6a0f05816be358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=256, continuous_update=False, description='Hidden dim', layout=L…"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    \n",
    "# Učitavanje programskih knjižnica\n",
    "from Skripte.Vjezba9.widget import deep_mlp_training_widget\n",
    "\n",
    "ui = deep_mlp_training_widget()\n",
    "ui\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
